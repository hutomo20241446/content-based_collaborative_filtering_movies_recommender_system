# -*- coding: utf-8 -*-
"""Movie_Recommender_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16tQBkUkT0DhysUcQNJLBk7EGddpmU0T7

# Load Data

Source yang digunakan didownload dari ['https://www.kaggle.com/datasets/abhikjha/movielens-100k'](https://www.kaggle.com/datasets/abhikjha/movielens-100k)
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("abhikjha/movielens-100k")

print("Path to dataset files:", path)

"""Ada dua dataset yang akan digunakan:"""

import pandas as pd

movies_df = pd.read_csv(path + "/ml-latest-small/movies.csv")
ratings_df = pd.read_csv(path + "/ml-latest-small/ratings.csv")

"""### fungsi dataset_summary()

fungsi `dataset_summary` akan digunakan untuk menampilkan informasi mengenai fitur-fitur, keberadaan nilai null, dan keberadaan nilai duplikat di setiap dataset.
"""

# Fungsi untuk menampilkan info dataset, jumlah nilai unik, jumlah baris dengan nilai null, dan jumlah duplikat
def dataset_summary(df, name):
    print(f"\n{'='*50}")
    print(f"\nINFO DATASET {name.upper()}")
    print(df.info())
    print("\nJumlah Nilai Unik pada Kolom:")
    print(df.nunique())

    # Menampilkan jumlah baris yang mengandung nilai null
    null_rows = df.isnull().any(axis=1).sum()
    print(f"\nJumlah Baris dengan Nilai Null: {null_rows}")

    # Menampilkan jumlah baris duplikat
    duplicate_rows = df.duplicated().sum()
    print(f"Jumlah Baris Duplikat: {duplicate_rows}")

"""### movies_df

Pada tahap ini dilakukan eksplorasi data pada movies_df.
"""

dataset_summary(movies_df, "Movies")

movies_df

"""**movies.csv (movies_df)**    
Dataset ini berisi informasi tentang judul dan genre dari dari 9.742 film. Tidak ada missing value atau duplikat pada **movies_df**. Dataset ini terdari dari 3 fitur, yaitu `movieId, title, genres`. Berikut penjelasan detail setiap fitur:

1. **movieId**  
   - **Tipe Data**: Integer ( Unique Identifier)  
   - **Deskripsi**: ID unik yang mengidentifikasi setiap film dalam dataset.  
   - **Contoh Nilai**: `1`, `2`, `3`, dst.  

2. **title**  
   - **Tipe Data**: String (Text)  
   - **Deskripsi**: Judul film beserta tahun rilisnya.  
   - **Contoh Nilai**: `"Toy Story (1995)"`, `"Jumanji (1995)"`.  
   - **Catatan**: Terdapat 9.737 judul unik, beberapa film memiliki judul yang sama.  

3. **genres**  
   - **Tipe Data**: String (kategori multi-label)  
   - **Deskripsi**: Genre film . Satu film memiliki lebih dari satu genre yang dipisahkan oleh tanda pipe (`|`).  
   - **Contoh Nilai**: `"Adventure|Animation|Children"`, `"Action|Animation|Comedy|Fantasy"`.  
   - **Catatan**: Terdapat 951 kombinasi genre berbeda, menunjukkan variasi yang cukup tinggi dalam pengategorian film.

---

Diagram di bawah ini menunjukkan distribusi genre pada film-film di movies_df.

Film-film yang mengandung unsur-unsur Drama, Comedy banyak diproduksi, masing-masing lebih dari  4000 dan 3000 film.
"""

import matplotlib.pyplot as plt

# Genre analysis for movies dataset
genres = movies_df['genres'].str.split('|', expand=True).stack().value_counts()
plt.figure(figsize=(12,6))
genres.plot(kind='bar', title=' Genres')
plt.show()

"""---

Daftar kombinasi genre dari masing-masing judul film dapat dilihat di bawah ini:

- Fitur genres mengandung sekumpulan genre yang ditulis secara terpisah menggunakan pipe. Pada bagian data preparation, pipe akan diganti dengan space agar memudahkan vektorisasi.
- Kombinasi genre, perbedaan total genre yang beragam di fitur genres menyebabkan nilai unik pada fitur genres sangat banyak (951).
"""

movies_df['genres'].unique()

"""### ratings_df

Pada tahap ini dilakukan eksplorasi data terhadap ratings_df
"""

dataset_summary(ratings_df, "Ratings")

ratings_df

"""**ratings.csv (ratings_df)**    
Dataset ini berisi rating yang diberikan oleh pengguna terhadap film-film tertentu. Tidak ada missing value atau duplikat pada **ratings_df**  Dataset ini terdiri dari fitur `userId, movieId, rating, timestamp`. Berikut penjelasan detail setiap fitur:

1. **userId**  
   - **Tipe Data**: Integer  
   - **Deskripsi**: ID unik yang mengidentifikasi pengguna yang memberikan rating.  
   - **Contoh Nilai**: `1`, `2`, `3`, dst.  
   - **Catatan**: Terdapat 610 pengguna unik dalam dataset.  

2. **movieId**  
   - **Tipe Data**: Integer  
   - **Deskripsi**: ID film yang diberi rating (berkaitan dengan `movieId` di `movies_df`).  
   - **Contoh Nilai**: `1`, `2`, `3`, dst.  
   - **Catatan**: Terdapat 9.724 film yang diberi rating, artinya beberapa film tidak memiliki rating.  

3. **rating**  
   - **Tipe Data**: Float
   - **Deskripsi**: Nilai rating yang diberikan pengguna pada suatu film.  
   - **Contoh Nilai**: `4.0`, `3.5`, `5.0`.  
   - **Catatan**: Terdapat 10 nilai unik (bernilai antara 0.5–5.0 dengan selisih antar nilai 0,5) .  

4. **timestamp**  
   - **Tipe Data**: Integer (Unix Timestamp)  
   - **Deskripsi**: Waktu saat rating diberikan, dalam format detik sejak **1 Januari 1970 (epoch time)**.  
   - **Contoh Nilai**: `964982703`, `964982296`.  
   - **Catatan**: Kolom ini bisa dikonversi ke format tanggal/waktu jika diperlukan untuk analisis temporal.

Catatan pada poin 3. rating di atas berasal dari daftar di bawah ini:
"""

ratings_df['rating'].unique()

"""---

Diagram di bawah ini menunjukkan distribusi rating yang diberikan user untuk film-film yang telah mereka tonton.

lebih dari 35% film mendapatkan rating tinggi ( rating >= 4)
- lebih dari 12,5% film mendapatkan rating 5
- lebih dari 8% film mendapatkan rating 4.5
- lebih dari 25% film mendapatkan rating 4
"""

import seaborn as sns

plt.figure(figsize=(10,5))
ax = sns.countplot(x='rating', data=ratings_df)

for container in ax.containers:
    ax.bar_label(container)

plt.title('Distribution of Ratings')
plt.show()

"""---

**movies_df** dan **ratings_df** akan digabung di bagian data preparation dan data gabungan ini akan digunakan dalam proses modeling.

# Data Preparation

## movies_new

Pada tahap ini `movies_df` dan `ratings_df` digabungkan untuk menghasilkan dataframe baru `movies_new` yang terdiri dari `'userId', 'movieId', 'title', 'genres', 'rating'`.
"""

# movies_new, gabungan movies_df dengan subset ratings_df berdasarkan movieId
movies_new = pd.merge(movies_df, ratings_df[['userId', 'movieId', 'rating']], on='movieId', how='inner')
movies_new = movies_new[['userId', 'movieId', 'title', 'genres', 'rating']]
movies_new

"""Kemudian, dari `movies_new` hanya dipertahankan data dengan kemunculan userId minimal 10 kali dan kemunculan movieId minimal 100 kali. Dataframe baru ini akan digunakan untuk proses selanjutnya."""

user_review_counts = movies_new['userId'].value_counts()
movie_review_counts = movies_new['movieId'].value_counts()

valid_users = user_review_counts[user_review_counts >= 10].index
valid_movies = movie_review_counts[movie_review_counts >= 100].index

movies_new = movies_new[
    (movies_new['userId'].isin(valid_users)) &
    (movies_new['movieId'].isin(valid_movies))
]

"""Dalam sistem content-based filtering atau collaborative filtering, menentukan batasan minimal jumlah review per user (userId) atau per movie (movieId) adalah penting untuk:

- Mengurangi data yang sparse (jarang) – Menghilangkan user atau movie dengan terlalu sedikit review agar model tidak belajar dari data yang terlalu noise.

- Meningkatkan kualitas rekomendasi – User yang hanya mereview 1-2 movie tidak memberikan cukup informasi untuk membuat rekomendasi yang baik.

## content_based_prep

### content_based_df

Pada tahap ini, dibuat dataframe baru `content_based-df` yang terdiri dari `'movieId', 'title', 'genres'`. Dataframe ini akan digunakan untuk membuat model content-based filtering.
"""

content_based_df = movies_new[['movieId', 'title', 'genres']].copy()
content_based_df

"""`content_based_df` terdiri dari tiga kolom `'movieId', 'title', 'genres'` dan 20188 baris.

### drop duplicates

pembuatan content_based_df menghasilkan data duplikat sehingga perlu di drop.
"""

# Pastikan data unik (hilangkan duplikat)
content_based_df = content_based_df.drop_duplicates(subset=['movieId', 'title']).reset_index(drop=True)

content_based_df.info()

"""Setelah penghapusan data duplikat, tersisa 138 rows data unik di `content_based_df`

### replace pipe dengan space pada genres, dan lowercasing

Pada tahap ini dilakukan replacement pada `genres`, pipe diganti dengan space dan huruf dibuat lowercase agar bisa dibaca TF-IDF dengan mudah.
"""

# Ubah format genre
content_based_df['genres'] = content_based_df['genres'].str.replace('|', ' ').str.lower()

"""Setelah replacement, dan lowercasing setiap genre dipisahkan oleh space dan semua hurufnya menjadi lowercase seperti pada sampel di bawah ini:"""

content_based_df

"""### genre_tfidf_matrix

Tahap ini bertujuan mengubah teks dalam kolom 'genres' menjadi representasi numerik berbasis TF-IDF dan menyimpannya dalam variabel genre_tfidf_matrix untuk kemudian dihitung cosine similarity-nya pada bagian modeling.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()
genre_tfidf_matrix = tf.fit_transform(content_based_df['genres'])

genre_tfidf_matrix

"""`genre_tfidf_matrix` merupakan matriks berisi nilai float yang terdiri dari 138 baris dan 18 kolom.

setiap kolom mewakili satu genre yang berbeda seperti yang dapat dilihat di bawah ini:
"""

tf.get_feature_names_out()

"""sampel matrix genre_tfidf_matrix dapat dilihat seperti di bawah ini:"""

pd.DataFrame(
    genre_tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=content_based_df['title']
)

"""## collaborative_prep

### collab_based_df

Pada tahap ini, dibuat dataframe baru `collab_based-df` yang terdiri dari `'userId', 'movieId', 'rating'`. Dataframe ini akan digunakan untuk membuat model collaborative filtering.
"""

collab_based_df = movies_new[['userId', 'movieId', 'rating']].copy()
collab_based_df

collab_based_df.info()

"""`collab_based_df` terdiri dari tiga kolom `'userId', 'movieId', 'rating'` dan 20188 baris.

### encode userId

Encoding `userId` dari  `collab_based_df` menggunakan fungsi `enumerate` untuk menghasilkan kolom baru `user`. Fitur ini akan digunakan sebagai variabel prediktor pada proses training.
"""

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = collab_based_df['userId'].unique().tolist()
print('list userId: ', user_ids)

#  Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userId : ', user_to_user_encoded)

#  Melakukan proses encoding angka ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mapping userId ke collab_based_df['user']
collab_based_df['user'] = collab_based_df['userId'].map(user_to_user_encoded)
print('encoded angka ke userId: ', user_encoded_to_user)

"""### encode movieId

Encoding `movieId` dari  `collab_based_df` menggunakan fungsi `enumerate` untuk menghasilkan kolom baru `movie`. Fitur ini akan digunakan sebagai variabel prediktor pada proses training.
"""

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = collab_based_df['movieId'].unique().tolist()
print('list movieId: ', movie_ids)

# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
print('encoded movieId : ', movie_to_movie_encoded)

# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

# Mapping movieId ke collab_based_df['movie']
collab_based_df['movie'] = collab_based_df['movieId'].map(movie_to_movie_encoded)
print('encoded angka ke movieId: ', user_encoded_to_user)

"""### inisialisasi variables untuk normalisasi & modeling

Inisialisasi  `num_movies`, `num_ratings` untuk modeling dan `min_rating`, `max_rating` untuk normalisasi `rating`ketika data splitting.
"""

import numpy as np

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)

# Mendapatkan jumlah movie
num_movies = len(movie_encoded_to_movie)

# Mengubah rating menjadi nilai float
collab_based_df['rating'] = collab_based_df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(collab_based_df['rating'])

# Nilai maksimal rating
max_rating = max(collab_based_df['rating'])

print('Number of User: {}, Number of Movies: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movies, min_rating, max_rating
))

"""### data splitting

- Memisahkan fitur prediktor dan target
   - Normalisasi target `rating` ke range 0-1.
   - Split data 80-20 untuk training-validation.
"""

# Mengacak dataset
collab_based_df = collab_based_df.sample(frac=1, random_state=42)
collab_based_df

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = collab_based_df[['user', 'movie']].values

# Membuat variabel y untuk membuat rating dari hasil
y = collab_based_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * collab_based_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Model Development Content Based

### menghitung cosine similary

Pada tahap ini dilakukan perhitungan cosine similarity genre_tfidf_matrix untuk mendapatkan derajat kesamaan antar film berdasarkan genre, lalu menyimpannya dalam DataFrame Pandas dengan indeks dan kolom berupa judul film untuk memudahkan analisis hubungan antar film.
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(genre_tfidf_matrix)

cosine_sim_df = pd.DataFrame(
    cosine_sim,
    index=content_based_df['title'],
    columns=content_based_df['title']
)

"""sampel cosine_similarity_df dapat dilihat di bawah ini:"""

cosine_sim_df

"""### fungsi movie_recommendations

Inisialisasi fungsi `movie_recommendations` untuk menghasilkan daftar film yang punya derajat kesamaan genre yang tinggi dengan film acuan yang diberikan pengguna.
"""

def movie_recommendations(judul_film, similarity_data=cosine_sim_df, items=content_based_df[['title', 'genres']], k=5):
    """
    Memberikan rekomendasi film yang mirip berdasarkan genre

    Parameters:
    - judul_film: str, judul film yang akan dijadikan acuan
    - similarity_data: pd.DataFrame, matriks kemiripan cosine antar film
    - items: pd.DataFrame, metadata film
    - k: int, jumlah film yang direkomendasikan

    Returns:
    - DataFrame: rekomendasi film
    """
    if judul_film not in similarity_data.columns:
        return f"Judul '{judul_film}' tidak ditemukan di data."

    index = similarity_data.loc[:, judul_film].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(judul_film, errors='ignore')  # Hilangkan film itu sendiri

    # Convert to DataFrame with proper column name
    closest_df = pd.DataFrame({'title': closest})

    # Merge with items
    return closest_df.merge(items, on='title').head(k)

"""### fungsi evaluate map

Inisialisasi fungsi `evaluate_map` untuk mengukur Mean Average Precision dari daftar film yang direkomendasikan oleh fungsi `movie_recommendations`
"""

from sklearn.metrics import average_precision_score

def evaluate_map(judul_film, k, df, similarity_data, items_df, threshold=4.0):
    """
    Evaluasi sistem rekomendasi content-based berdasarkan Mean Average Precision (MAP)

    Parameters:
    - judul_film: str, judul film acuan
    - k: int, jumlah rekomendasi yang akan dievaluasi
    - df: pd.DataFrame, data rating (dengan userId, movieId, title, rating)
    - similarity_data: pd.DataFrame, matriks cosine similarity antar judul
    - items_df: pd.DataFrame, metadata film (title dan genres)
    - threshold: float, batas rating untuk dianggap relevan (default: 4.0)

    Returns:
    - dict: hasil evaluasi berupa MAP
    """
    # Ambil movieId dari judul
    try:
        movie_id = df[df['title'] == judul_film]['movieId'].values[0]
    except IndexError:
        return f"Judul '{judul_film}' tidak ditemukan di data."

    # User yang menyukai film tersebut
    relevant_users = df[(df['movieId'] == movie_id) & (df['rating'] >= threshold)]['userId'].unique()

    # Ambil semua film yang disukai user tersebut (ground truth relevansi)
    relevant_titles = df[
        (df['userId'].isin(relevant_users)) &
        (df['rating'] >= threshold)
    ]['title'].unique()

    if len(relevant_titles) == 0:
        return f"Tidak ada ground truth relevan untuk '{judul_film}'. Evaluasi dilewati."

    # Ambil hasil rekomendasi
    recommended_df = movie_recommendations(
        judul_film=judul_film,
        similarity_data=similarity_data,
        items=items_df,
        k=k
    )
    recommended_titles = recommended_df['title'].tolist()

    # Buat label relevansi biner
    y_true = [1 if title in relevant_titles else 0 for title in recommended_titles]
    y_score = [1.0 / (i+1) for i in range(len(recommended_titles))]  # bobot berdasarkan posisi

    map_score = average_precision_score(y_true, y_score)

    return {
        "Judul": judul_film,
        "MAP": round(map_score, 4)
    }

"""### inference

menggunakan "Toy Story (1995)" sebagai film acuan.
"""

# Cek apakah film tersedia
content_based_df[content_based_df['title'].str.contains("Toy Story (1995)", case=False, regex=False)]

"""daftar rekomendasi sepuluh film yang memiliki nilai cosine similarity tertinggi dengan Toy Story (1995)."""

# Rekomendasi film berdasarkan "Toy Story (1995)"
movie_recommendations(judul_film="Toy Story (1995)", k=10)

"""hasil evaluasi dari 10 film yang direcomendasikan menunjukkan score map yang memuaskan sebesar 1,0."""

evaluate_map(
    judul_film="Toy Story (1995)",
    k=10,
    df=movies_new,
    similarity_data=cosine_sim_df,
    items_df=movies_new[['title', 'genres']]
)

"""# Model Development Collaborative Based

### membangun RecommenderNet

Pada tahap ini dibangun arsitektur model neural network RecommenderNet berbasis tensorflow dengan empat embedding layer—dua untuk pengguna dan dua untuk film—yang direpresentasikan oleh vektor embedding berdimensi rendah (embedding_size). Selain itu, ditambahkan bias embedding untuk menangkap kecenderungan pengguna dan film dalam memberikan atau menerima rating lebih tinggi atau rendah dari rata-rata. Semua layer embedding diinisialisasi dengan metode he_normal dan diberikan regularisasi L2 untuk mencegah overfitting.
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movies, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movies = num_movies
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.movie_embedding = layers.Embedding(
        num_movies,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movies, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    movie_vector = self.movie_embedding(inputs[:, 1])
    movie_bias = self.movie_bias(inputs[:, 1])

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""### compiling & training model

Model di-compile menggunakan fungsi binary crossentropy sebagai loss function, dan Adam optimizer untuk proses pembelajaran, dengan metrik evaluasi berupa Root Mean Squared Error (RMSE).

Proses pelatihan dilakukan selama 40 epoch dengan ukuran batch 8, serta dilakukan validasi terhadap data validasi (x_val, y_val) untuk memantau performa model.
"""

model = RecommenderNet(num_users, num_movies, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 40,
    validation_data = (x_val, y_val)
)

"""### visualisasi performa model saat pelatihan

Visualisasi ini digunakan untuk memudahkan interpretasi performa model disetiap epoch selam pelatihan.

Gap antara nilai rmse train dan rmse validation mulai epoch ke 20 relatif stabil. Selisihnya pun kecil, hanya sekitar 0,025
"""

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""### daftar film di rating oleh user acuan user_id

`user_id` adalah salah satu user yang dipilih secara acak.
`user_id =  collab_based_df.userId.sample(1).iloc[0]`

Daftar ini diurutkan dari film dengan rating tertinggi.
"""

# Untuk membuat rekomendasi movie
movies_df = movies_new[['movieId', 'title', 'genres']].drop_duplicates()

# Mengambil sample user
user_id = collab_based_df.userId.sample(1).iloc[0]
movies_watched_by_user = collab_based_df[collab_based_df.userId == user_id]

print('----' * 8)
print('Movies with high ratings from user')
print('----' * 8)

top_movies_user = (
    movies_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

top_movies_user = movies_df[movies_df['movieId'].isin(top_movies_user)]
top_movies_user

"""### inference

Daftar rekomendasi 10 film berdasarkan film-film yang telah dirating oleh user 'user_id'.
"""

movies_not_watched = movies_df[~movies_df['movieId'].isin(movies_watched_by_user.movieId.values)]['movieId']
movies_not_watched = list(
    set(movies_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movies_not_watched = [[movie_to_movie_encoded.get(x)] for x in movies_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)
)

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]
recommended_movies

"""---"""